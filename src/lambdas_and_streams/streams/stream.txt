 Java 8 introduced streams API for processing and calculation functionality on data. It is a brand-new shiny API that lets us work with creating a pipeline of operations on a sequence of data streaming through the data source. So what is a stream? And why is it used? How can we use streams with lambdas? So in this session, we will go through the general introduction of streams. Say we have a list of trades and we wish to find out set of large trades, trades whose quantity is more than 10,000. We can satisfy this requirement by both ways, one, using streams, and another, pre-Java 8, way. So let's first see how we can do without using streams. We have a method here for finding out a list of large trades given a collection of trades by the client. Let's use traditional imperative programming style. So we begin here by creating an intermediary collection. largeTrades is this collection, which holds trades larger than 10,000. We run through each of the elements of the given set of trades, check if the trade satisfies the large trade condition, and add that trade to the intermediate collection so we can return the collection of large trades once the program completes its iteration. Before we go onto applying shiny streams, let's see what we are doing here. Is it, anyway, bad or not? To begin with, the code is using a throw-away array list, which consumes the memory. For example, largeTrades might be running into 10,000, maybe a million trades. And also, we have an input list of trades which might have another million trades as well. Imagine the storage implications should the large trades amount to millions? We shouldn't be creating this intermediary storage. Unfortunately, the programming style doesn't give that choice to us. Secondly, did you notice we are iterating through the trades list one after the other? Developers are explicitly expected to write the iteration code. This brings another interesting concept of how to do it versus what to do it. The expectation is that libraries should deal with how and when to do a certain task, while users should only be worried about what to do. Unfortunately, this separation of concern can't be marked clearly in such code. Next, we can see that the code is written for a single threaded execution model. There are few moving parts in there. Should we wish to parallelize this code, maybe to do an increase of dataset from thousands to billions, we certainly need to start reengineering the code base. And we all know how difficult it is to get a multi-threading code correct. Lastly, what if we have to throw in a few other conditions, new conditions, for example, because our client has changed his mind? For example, only cancel trades and large trades identified by a particular issuer, or perhaps trades placed by a particular trader-- we need to write lots of control statements, such as, if as, or logical and or logical or blocks, don't we? So we are forced to write a lot of control statements and logical and blocks. This isn't a great way of expressing our intent as well. So is there a better way to get rid of these wrinkles? Can we parametrize the behavior? Can we not hardcode this behavior into the code? Or can we not apply the behavior using our new friend, lambda? Fortunately, the new streams API comes into picture for this exact purpose. We'll take a break here and come back and see how we can use streams API to tackle these issues in our next session.

  In the last two sessions, we looked at the problem domain streams and solving. We barely scratched the surface of streams API in those two sessions. In this lesson, we are going to consolidate our understanding of streams by looking at a few more examples. To recap, a stream is a free-flowing sequence of elements. The data flows through continuously, incessantly. Streams do not hold any storage, as that responsibility lies with collections, such as address, lists, and sets, or other data sources. Every stream starts with the source of data, sets up a pipeline, process the elements through the pipeline, and finishes with a terminal operation. Streams have this extremely useful paralyzing feature. We could put that to work when we are working with data intensive operations. A new package, java.util.stream was introduced in Java 8 with Stream as one of his core interface. Streams adhere to a common pipes and filters software pattern. A pipeline is created with the data evens flowing through with various intermediate operations being applied on individual elements as they move through the pipeline. The stream is set to be terminated when the pipeline is disrupted with a terminal operation. Look at the picture here. We have a source. A source can be a simple CSC file or a hash map or a database which produces a stream of data. Now, we need to do some operations on this stream. For example, filtering the elements into a basket based on a particular condition or perhaps slicing the elements with some user defined predicates, or maybe transforming the elements into other entities, we apply these operations on the elements as they are spit out of the data source. These operations are called intermediate operations. They can be many intermediate operations. Their input and the output of an intermediate operations is always a stream itself. The final operation is what ends the stream. It's called a terminal operator or terminator. This could be functionality such as counting the list or collecting the elements into a container, or simply printing out them to the console. So the stream is a series of elements splitting out of the source followed by application of various intermediate operations, and finally produces a result, a terminal operation. Do note that a stream is expected to be immutable. So any attempts to modify the collection during the pipeline will raise a concurrent modified exception. Now, let's go back to Eclipse and consider an example of collecting list of movies that are directed by a particular director, Steven Spielberg. First, let's start with a simple use case. We spit out all the movies using the stream EPA. What we have done here is opening up a stream on collection, which is a list of movies. Remember collision interface is enhanced to work with streams, and that is what we have used here. And then we have a dash to ForEach operator at the end of the stream. The ForEach is a terminal operator, meaning you can't have any other operations. After that, the whole process will be finished. If you try to access the stream after this, you get an exception, as stream was already exhausted. Now, the Able process created a pipeline, although no real data processing is happening before the final operation. So we enhance the pipeline with a few more data manipulation techniques and calculation functions, such as filtering, mapping, and limiting to a certain number. We wish to check the movies are directed by Steven Spielberg, so we must use a filtering technique. So we have created a filter here. Given a movie, we find out the data for the movie, and obviously if it matters to Steven Spielberg, that means that movie is going to be printed out in our next line. Let's execute this program and see what happens. As you can see Steven Spielberg's classic movies are produced here. If I take a red filter, you see a list of movies, not just Steven Spielberg movies, but we just need a movie name. So we have another requirement here, getting a name of the movie, but not detailed version of movies. What we do is that we add another function and mapping function, which extracts the name of the movie given the movie object. We are using a method reference, so we have a getName method on the movie, so we are simply using a shortcut lambda here. So if we execute this function, what happens? The movie details are gone and we simply get a name of the movie printed out. Let's say we only want to limit the list to three. How are we're going to drop the rest of the movies? Using Limit operation. As you can see, the rest of the movies will drop and we are limited to three movies printed out. Now we have two Jurassic Parks mentioned here. That is, we are producing a duplicate movie. So how can we eliminate this? What we can do is that we could also use something called distinct. You can see that we are producing three movies removing all the duplicates. So the pipeline can be enhanced based on the slicing and dicing or transformation, or any other requirements of the client's choice. See how explicit the code is here. In this session, we looked at streams a bit deeper, we created a simple pipeline, and enhanced according to the changing requirements.


 Streams API supports invoking various operations, such as filters, mapping, collectors, and so on. Although these methods look plain and simple, they come with a special characteristic that is laziness or eagerness in them. In this session, we learn about these characteristics of laziness and eagerness using some examples. Any method that executes straightaway is called as an eager method. The opposite of this is loosely what defines the laziness. The method won't get executed unless it's been explicitly asked to do so. If you are familiar with the object relational management framework, such as Hibernate, you would know we employ eagerness and laziness as tactics for performance benefits. Clients will be provided with a skinny version of the domain object only to fetch the full object from the database once clients ask for a detailed version. This use case particularly works well for client searching requests on a website. In the context of streams, laziness is to defer the execution on a pipeline until it has been asked by a terminator operation. Streams are built using pipes and filters pattern. A pipeline of activities are clubbed together and executed when asked to do so. So let's walk through examples. Say we have a list of movies. Suppose we create a method that works out some business logic on the movie stream. Here we are applying a filter which tests the condition for movie being a classic or not. If you execute this method, the output will not get printed out. So let's add a system or print out statement to the filter as well. So we have a filter operation here which simply checks the movie is classic or not. And returns it, but also it prints out if we are in that filter. So let's execute this class and see what the output is going to be. There's no output, although the system has run this application successfully. There is nothing wrong with this code. It's just that the operation helps creating a pipeline that doesn't get executed until another operation is executed, let's say, a count. You can see that immediately, we know that all the filter operations have been outputted to the console. Count method kicks off the pipeline and executes it. So why does a count method starts to work by doing something while the filter or map or any of the intermediary operations doesn't start off the pipeline at all? This is where the difference in laziness and eagerness comes into picture. The filter method is simply a lazy method. It can't produce any result apart from creating another stream. There are other methods that fall into this category, such as map, filter, sorted, peak, distinct, limit, or skip. They are lazy operations. The output of an intermediate operation will always be a stream. In fact, this is one of the best ways to find out if the operator is an intermediary operation or not. The terminal operators, on the other hand, return a value, or at least void. Methods like count, per each, find first, find any, order and match methods like all match or none match, they will execute the pipeline and produce a result. This is, indeed, the litmus test to check if the operator is, in fact, a terminator. Let's see an eager method in action. I simply copy and paste a filter and a map in here. As you can see, we are invoking filter method on the movie stream as well as map method too. The only difference is that inside the filter, we are trying to manage a print out statement for both map and filter as well. Let's see if we can run this program. We can see the mapping and the filtering being invoked because of this particular method count, which is a terminal operation. So here we have a filter and map both coded with the print statement. When we have a count method invoked here, we can see that both these print statements are printed out. This is because a count method is a terminal operator. And all terminal operators do is invoke the pipeline and produce a final result.

 streams vs collections:
  Streams can be created from various collection data structures such as list, maps, or sets. On the high level, both streams and collections may look more or less the same. In this session, we learn about what are the fundamental differences, when and where we should be using both of them. Collections are simply data structures that deal with storing elements. Storage is their primary obligation. They provide contract around insertion and accessing of these events in most efficient manner. For example, although array list and linked lists both can hold data, we choose array list over linked list if positional accessing of elements is the use case. If speedy insertion or removal of data is a requirement, then we may choose linked list over array list. They can also have a contractual binding. Before the element is inserted into a collection, it may have to be computed as well. On the other hand, streams don't have any storage whatsoever. They depend on a source of data. And that source can be a collection, or a file, or something else. Without a data source, they are nothing. The other difference is [? collection's ?] [? expert ?] use is to explicitly iterate through the elements, while streams do not. In collections, we employ external iteration strategy, meaning users are expected to iterate through the data, while in streams, users never interact with iterations at all. This strategy is internal to the library called internal iteration strategy. The next differences are collections will always have a finite amount due to the physical storage underlying them. As streams are free from storage implications, they can be designed to be in finite streams. For example, should we wish to provide a list of negative numbers to a function, we simply use either generate or iterate methods available on the stream interface. These methods produce a continuous stream of elements, hence they are called infinite streams. Streams have other sources too, not just collections. Streams can be generated from files, set of simple values, or even custom sources. Streams form a pipeline using a data source with intermediate and final operations. The collections just fit in as the data source in this pipeline, nothing more. Lastly, the data element gets calculated or computed when inserting in a collection, but in streams, the computation happens dynamically when you are pulling out the data from the streams, more like an on-demand basis. So to summarize, the streams are sequence of elements applying various transformations, filtering logic on individual data elements, while collections are data storage structures standing in for one of the data sources on these streams.

  primitive streams:
   Most of the time, we wish to work on primitive values rather than the references when working with streams. For example, if we wish to find out the sum of quantity of all the trays, this is what we will do. Here we have streamed the trades, fetched the quantity, and reduced the quantities to a sum, a single value. Reduce is an aggregation function, a function that would accumulate all the trade quantities to produce a final sum, as seen here. Did you notice a return type? Optional is a new type introduced in Java 8 to combat famous null point issues. We will work on optional type soon, but let's continue to work on primitive streams. The approach seems to be all OK so far, but the issue is around cost of boxing that this block of code has to incur. There is a considerable cost in conversion of primitive int to integer class, and vise versa. The quantities are worked out as primitive integers, but converted to integer objects, and so on, behind the scenes. Designers have delivered a stream that works on primitive integers, primitive longs, and primitive doubles too. There are three specific implementations of primitive streams, an int stream, long stream, and a double stream. As the name suggests, they support the respective primitive types. Let's quickly look at int streams. We create an int stream by simply calling a static method of on the stream. The method of is an [INAUDIBLE]. We can either provide a set of integers or an object of integer array. This is the static of method taking all the integers, producing an int, intStream. Now you have intStream, you can simply use that for producing the values out. If you run the program, you see the integers produced has primitive integers printed out on the console here. We could also provide a list of integers like this. Here the of method is considering the integers straight away rather than an object of arrays, like shown earlier. Both should print out exactly the same values to the console. Of course, I missed out 10 in the second one. The same principle follows for double and long streams too, so I simply copy the code. We have here an array of doubles. So using double stream of method, we pass that array object to this method to produce a double stream. And as you can see, we are printing them out first. We also have a second of method on double stream, which takes in doubles, as expected. That also produces the same double stream. Let's run this program. It produced a double stream as well. We can also convert object streams into primitive streams or vise versa. If we consider our earlier example of aggregating trade volume for a given set of trades, what we need to do is invoke the mapToInt method on the object stream. Let's see how we can do this here. This produces a stream of trades. And invoking mapToInt, calling a function quantity, produces an int stream. So this mapToInt is simply producing an int stream. We can now use a sum method expressed on the int stream to get an aggregated quantity directly. This produces an aggregated sum of all the trades of quantities. So the benefit of doing this is to avoid boxing costs. Let's wrap this session here. We learned about how we can work with primitive streams in this session. There are a few methods exposed on the primitive streams API. We will run through them in coming sessions.


   termintators are eager operations (they produce output) and intermediate operations that produce streams are lazy operations which themselves don't produce any output.

   collector is a new interface introduced in Java 8 alongside of streams interface. For now, let's concentrate the task at hand. A utility class called collectors has been used to ease our life. Make sure you understand the difference between the collector and the collectors. Collector is a interface, whereas the collectors is the utility class. So we simply use the utility methods exposed by this class to collect the data stream

   short-circuit ops
    Collecting the data into an appropriate container tells only one side of the story. Searching through elements and finding the one which you are looking effectively is another story. Java 8 provides finder functionalities via streams API. This lesson is all about matching and finding the data. There are a handful of methods that would fit for matching and finding data elements. While findAny and findFirst methods search and find the elements, the matching methods, such as anyMatch, allMatch, noneMatch, are used to check the elements against a predicate, returning a true or false result. These are also termed as short-circuit operations, meaning the pipeline processing will seize once the element is found. They carry a great performance benefit, because they don't have to see through the entire population. Say we like to fetch the first available big trade out of entire population of trades. We use findFirst method to satisfy this requirement. We filter through the trades to find a big trade. Once a big trade is found, the findFirst will return it and stop executing the pipeline further. There is a chance that defineFirst might not yield any fruitful results. So in such cases, a null pointer exception might be thrown. To avoid such issues, findFirst returns an optional, as you can see, the return type being an optional of trade here. Remember, option is a new type that may or may not have a value. It's a safety guard against null pointers. If our intention is to find any of the big trades, not just the first one, then we invoke findAny method. Let's see how we can do this one. I will copy the same code base here and change one method from findFirst to findAny. So what is the difference between these two methods? You may have noticed a similarity in findFirst and findAny methods. They are pretty much the same, except they behave slightly different when executed parallelly. If say, you are executing this code parallelly, the findAny will return any trait that satisfies the big trade condition instantly and aborts the rest of the code execution in any other thread, whereas the findFirst will have to wait until all the detail is returned before checking for the first available result. So if you think of the code block as the right candidate for parallel run, then use the findAny method and stick to the findFirst method. While the findFirst or findAny returns the element satisfying the conditions, the anyMatch, allMatch, and noneMatch methods check if a condition is satisfied, returning a Boolean value. These methods take in a predicate, unlike the finder methods. Lets see the anyMatch. The anyMatch method is used to check if the element satisfies a given predicate. If you wish to know a rotten trade availability in a basket of trades, then we use anyMatch, as shown here. We sift through all the trades and check for the status called ROTTEN on each of them. The anyMatch will find out the trade which has called the status as rotten and return you a true value. If the status matches the given predicate, instantly the pipeline will return a Boolean value. Did you notice the anyMatch argument, it takes a predicate? Suppose we wish to find if all trades match a certain criteria, then we use allMatch method. Let's see how we do that one here. Let me copy the code. So this code checks if all trades are IBM trades. Say we like to find out if none of the trade is a canceled trade, how do we do that? We use noneMatch method, passing in our predicate. So let's see how we can do that on here. The predicate checks the cancelled trades and accordingly outputs the Boolean value if one or the trade is indeed a canceled trade. Let's run this program. To summarize, finding and matching of elements against a stream of data can be done by short-circuit operators using finder methods or match methods.

    reducing:
    The Stream API exposes a method called reduce to aggregate a single value from a set of values. There are three variants of this method, each of them serving a particular use case. Let's look at one which takes a single argument, a type of binary operator, as its argument. Binary operator is a specialized bifunction function that consumes two operants of the same type, returning the result in as the same type two.
     Stream support reduction operations. Reducing is a way of aggregating to a single result. In this lesson, we look at these reduction operations. The Stream API exposes a method called reduce to aggregate a single value from a set of values. There are three variants of this method, each of them serving a particular use case. Let's look at one which takes a single argument, a type of binary operator, as its argument. Binary operator is a specialized bifunction function that consumes two operants of the same type, returning the result in as the same type two. So if we have a function that takes in two integers returning an integer, we can represent binary operator with an integer has this genetic type. The reduce method takes in a binary operator. Let's take an example. We wish to output all the instruments of the trades that we have as a comma separated string formatted values. First, we need to extract each instrument from each of the trades. So we use a map function for this. And we need to output all the instruments as a single string, enabling us to use reduce operation. As you can see, the reduce method is taking a lambda expression. The lambda is of type binary operator. So let's first run this example and see the details behind. As you can see, we are getting comma separated instruments on these trades. The way reduce function works isn't complex to understand. The first element from the stream is added to an initial element. In this case, the initial element is not defined. So it's empty. The result of this operation is then added to the second element of the stream. The result of the second operation is then added to the third element of the string to produce another result. Third operation is fulfilled. This continues until the stream is exhausted and the final result return back to the user. Did you notice the return type? The value is an optional type. The optional type is a new type introduced in Java 8 for handling null pointers effectively. Let's take another example. We wish to find out the sum of quantities of all trades in our trades list. I'm just going to copy the existing code. We take the quantity of each and every trade and use reduced function to add that quantity. As you can see, we achieved a total sum of quantity of all the trades. The quantities extracted from each trade and given out to the reduce operation, which then accumulates the value by summing up the quantities one by one. The overloaded method of reduce takes in an initial value too, along with the binary operator. So let's whiz through an example. We have a list of children in a primary school here. We wish to find out the total strength of school including staff, along with the children. We know the staff is 10. So we do this as shown here. We present the staff number as a initial starting point, and then provide the lambda for aggregating the sum. Did you notice the return type again? This is an optional. So when we provide an initial value, the expectation is that we will receive a final definitive outcome. So chance of null pointer being nil. Let's wrap this up here. In this session, we have learned about reduction operations. Learned about how to use method can be used to aggregate or sum up values.

     Reduce operation may yield nothing, in which case data processing would produce the null pointer exception. In order to handle this scenario the result is enclosed in an optional class which is nothing more than a container to hold the data.


     Partitioning is a specialized function for creating classification based on a Boolean key. We have seen examples on how to create partitioned maps as well as nested partitioning.

     Collectors:
     Collectors are reduction operators, operators that were reduced to a single value and collected into a container. We learned about basic operations like collecting to lists, sets, or maps, as well as grouping and partitioning operations on the data by invoking the collect method earlier on on the streams. All these operations are provided by the inbuilt utility class called Collectors. This is a newly created, handy, class with some static methods to ease our life. Don't get confused with Collectors and collector. Collector is an interface, whereas as Collectors is a final utility class. The collect method on the stream interface takes in a collector instance. So Collectors class implements the required functionality, returning a collector instance, as expected by the collect method. It was designed to help us in providing default collection algorithms such as toList, groupingBy, partitioningBy, toMap, et cetera. If we wish to create our own collector, all we need to do is to implement a collect interface and pass it to the collect method. If the out-of-the-box utility methods [INAUDIBLE] Collectors do not suffice our requirements, sure we can implement our own collector, too-- collect it to store the trades in a trade data structure, or when a collector to contain all movies in a particular movie collector as well. So this lesson briefly run through the details of how we can implement our own custom collectors. It would be a good idea to run through a collector interface briefly. Collector interface provides the functionality to accumulate elements into a container, optionally applying transformation function on each of the element. It's a mutable reduction operation that can run sequential or parallel. For example, we wish to create a long list of comma separated instruments from a given set of trace, horror movie container where each element is a movie itself. Collector interface has five methods, as you can see here-- a supplier, an accumulator, combiner, finisher, characteristics. Each one of these methods has a certain function. The supplier is a container. It's going to create new container for the elements which we are going to store. Accumulator, on the other hand, is something that will insert the elements into this newly created container by the supplier. Combiner is an aggregator, where it combines or merges two intermediate results to produce a final result. We can omit creating a combiner if we know that, for sure, our code will not be processed parallely. That is, if our code is sequentially run, then we don't need to provide a combiner. However, if you execute a program in a parallel mode, there's going to be an error thrown if we don't supply a combiner implementation. Finisher is a transformation function that's applied on the elements, moving from type A to type B. The last method, characteristics, is basically the information to the runtime if the program is executed to be run in a parallel mode or a sequential mode. Let's walk through an example by building a custom collector, a collector that would collect simply all the instruments off the trades into a common, separate String. The collect method takes in three parameters-- it's supplier, accumulator, and combiner. First rule is to find out the right container. What we need here is a String representation of the instrument values, right? That means our container should hold Strings. Maybe a StringBuilder would do the trick for us. That means our supplier will become a supplier of StringBuilder, something like this. This creates a new StringBuilder. We are using a conceptual reference to create a supplier itself. Supplier is a function which creates StringBuilder instance. The next piece is the accumulator. The purpose of accumulator is to consume each and every element of the stream and add them to the newly constructed container-- StringBuilder, in this case. The accumulator is a bi-consumer. Hence, the call should be like this. For a given trade, we extract the instrument and append to the given StringBuilder, followed by a comma, which means we are creating a String of comma-separated instruments and attaching it into the container, which is our StringBuilder. We are simply accumulating the names into a container. Remember that. Lastly, a combiner is used to merge the results for each chunk calculated by other threads. As one can imagine, the combiner is expected to combine the mini-containers to merge their result. So the input [INAUDIBLE] surely would be two containers, that is, two StringBuidlers, to be precise. Another point to remember is that the combiner is also a type of bi-consumer, so the call should be like this. Given two StringBuilders, we are simply copying the elements one of them into the other one. So let's finish this example by streaming the trades and invoking the collect method, passing these three arguments. We are collecting using second variant of the collect method, which takes in a supplier, accumulator, and combiner. The return type of this one is going to be our new container, which is basically nothing but a StringBuilder. Let's print out the result. Run this program, and check the output. As we can see, all the instruments of those four trades are output to the console as comma-separated values. This is the second variant of the collect method. It requires three input parameters-- a supplier, accumulator, and a combiner. In our earlier sessions, we also seen something called minimum, maximum, average, including the size of a stream of elements. Java 8 used a new class call IntSummaryStatistics, which gives you exactly these details rather than recording up using min, max, or average. Let's see how we can do this one straightaway. I simply copy the code. I have a stream of trades here. We are going to invoke a method call summarizingInt when we are collecting the data. So this method, collectors.summarizingInt, will return an IntSummaryStatistics instance. What we are trying to do here is get the quantity of each and every trade off the stream and do the summary of each of them, and produce statistics of those values. Let's see the output. This particular class has already created statistics. So for example, the stream has got four elements. So that means you have a count. The total value of the quantity of the trades across all those four trades is this much. So is the minimum, and the maximum, and [INAUDIBLE] average. So this is a very, very handy class for getting the summary statistics of your stream. We looked at collect method earlier, which was used to collect the data into a list or a set or a map. This time, we used a different collector, a collector that summarizes the statistics. The return type is an object representing the stats. This class encapsulates all the statistic functions in one go. In this lesson, we learned about custom collectors. These are handy collectors created by us for our own specific needs.

Flattening:
 Flattening the objects is certainly a requirement for our real world programs. We may have a list of movie genre that would have a list of movies in each, we may have a list of actors. Say we wish to produce a list of all movies by collecting from each of the genre. However, flattening this object graph is a cumbersome process, as we may have to use four loops inside loops in Java programming language until Java 8. Let's see this as an example here. We first browse through movies. And for each movie, we are to chose the list of actors and then bring them out to the console. Isn't it a bit in one process? Well, we can also use something called FlatMap for such purposes instead. FlatMap, as the name suggests, will flatten out the contents and produce a new stream of those elements. In this case, we reflect the movies to get a stream of actors. Let's see how we do this using our stream's API. The FlatMap will take in a map of function. Here, the FlatMap takes in a function that would extract the actors from the given movie, producing an actor stream. Less run this example. As you can see, now we have a stream of actors rather than having a list of movies, each movie having a list of actors. Let's take another example. Say we have two-string arrays, arrays of vegetables and arrays of fruit, and we like to produce a stream of fruits and veggies from these two arrays. So, first, will produce a stream by using stream of function. Here we have converted the fruit into a list, veggies also into a list, and used the stream functions of method to produce a stream. However, because these are lists, the output of this one is going to be a stream of lists. Now, this is not what we want to write. We wanted to have just a stream of strings, rather than having a stream of lists, which will have another list of strings. So this is where FlatMap comes into picture. We simply use FlatMap on this particular stream by providing a map of function. The map of being anything which is coming in here, convert it into a stream. Let's do the printing out, as well. Now let's run the program. As you can see, you have a list of strings return as just a stream of strings. So to summarize, a flat map is used for creating a stream of flattened elements. It's a quite useful stream operation, which easily converts the object graph into a stream.

 peak methods are used for debugging

 serial vs parallel:
  Switching a program execution from sequential mode into a parallel one is really straightforward. All we have to do is invoke parallel method when needed. By doing so, we straightaway get the benefit of using the multicore. On the pipeline, we can switch between the sequential and parallel mode if we want to very, very easily. Because parallelism comes for free out of the box, does it mean that we can simply use a parallel mode execution for each and every program using the multicores to the maximum? Isn't it turning a single-threaded process on a lone core to a multicore more advantageous? Well, not exactly. Although the toolkit is just under our belt, creating the parallel programs without a base is going to hurt. This lesson, we will learn the dos and don'ts of parallel execution. We'll look at the things we need to consider when thinking of moving to multicore programming mode. We looked at how the data will be split into chunks earlier. Each chunk queued to a thread, producing a result. The results are then merged or aggregated, resulting to a final answer. The type of the data source plays an important role when the job is split into chunks. For example, if your data source is an added list, it's quite easy to split the data as the data structure supports random access, whereas if you consider a linked list, splitting of the data is very, very inefficient. Accessing the data from a linked list is quite poor. The next big thing we need to consider when working with parallel programming is the size of the data. There's always an overhead when creating multiple threads, managing them, and merging the data back from them. This overhead is justified if we know that the input data is huge and, of course, splittable easily as well. You may be wondering what is the size we should start thinking of splitting the work into mini jobs? Well, there's no definite answer to that one. The best answer will be derived from your own measurement test. Let's talk about measurement tests for a minute. Neither the serial and parallel code are identical apart from parallel method sticker. The measurement is actually a tad bit easier for both the types of programs. Let's write a simple program to check how we can measure the performance of a piece of sequential and parallel code on a dev machine. Keep in mind, this program is going to run on a quad-core MacBook laptop. Say we have a list of trades, and we wish to sum of the quantity of all trades. Let's do that in sequential mode first. Here, we have a list of trades with a large set of trades being created in that list. I have a stream already created for us. So let's create a map which extracts quantity of the trade and then reduce it to add the quantity to the main value. Here, we are extracting the quantity of each trade while the mapping function and adding them up with a reduced function. As you can see, there's no parallel invocation method used. Let's use simply 100 trades. In order to capture the time, I'm using a new class introduced in Java 8. That's Instant. So Instant, we are trying to call before and after the execution. We take the difference of start and end using another class called Duration, which will provide us the time and use d.toMillis, which will give you the milliseconds in difference. Let's run this program and see the output. As you can see, aggregating trades took 76 milliseconds in sequential mode. Now let's turn this program into a parallel mode. Remember, all we need to do is add a parallel method onto the pipeline. I simply copy the pipeline and invoke parallel of that. Now with 100 trades in stock, let's run the program and check out the trades. So it took about 168 seconds in sequential mode, whereas parallel mode, I think we have suppressed the invocation. Less rerun it. So it took about 30 milliseconds for parallel mode, whereas 94 milliseconds for sequential mode. So that means 100 trades have been split up into four groups because I have four cores. Normally, per core, you created a thread. And they've been mapped as well as reduced on those four threads. Sometimes there is a chance that the sequential might be better off than parallel. Change the size of the trade population and retry them both, sequential and parallel. You may notice the bigger the population will favor the parallel behavior. Coming back to our slides, we have to learn one more thing. We learned earlier, that some operations on the pipeline are stateless as well as stateful. The stateless operations, such as filters and maps, can seriously improve the performance of the parallel operations as they only require the current element and don't bother of earlier ones or even entire set of data. Stateful operations, such as limit or distinct, or when sorted on the other hand, they slow down the benefits of using parallelism. The reason is that these operations may have to wait until they see the entire population rather than acting just on the given element. Also do note that when you are using operations, like limit or find first, the ordering is important here too. Hence, sequential processing is undertaken. So to summarize, when leaning towards parallelism, do your homework first. Make sure you check the size of the data, data structures, pipeline, which has got stateful or stateless operations, and also measurement tests with some samples of data.